const express = require('express');
const { createProxyMiddleware } = require('http-proxy-middleware');

const app = express();
const PORT = 1234;

// Funci√≥n para detectar autom√°ticamente el puerto de LM Studio
async function detectLMStudioPort() {
    const possiblePorts = [41343, 1234, 8080, 3000, 5000];
    
    for (const port of possiblePorts) {
        try {
            const response = await fetch(`http://localhost:${port}/`);
            if (response.ok || response.status === 404) {
                console.log(`üîç LM Studio detectado en puerto: ${port}`);
                return port;
            }
        } catch (error) {
            // Puerto no disponible, continuar
        }
    }
    
    console.log(`‚ö†Ô∏è LM Studio no detectado en puertos comunes, usando 41343 por defecto`);
    return 41343; // Puerto por defecto
}

// Variable global para el puerto de LM Studio
let LM_STUDIO_PORT = 41343;

// Middleware para logs
app.use((req, res, next) => {
    console.log(`${new Date().toISOString()} - ${req.method} ${req.url}`);
    next();
});

// Funci√≥n para verificar si LM Studio est√° disponible
async function checkLMStudio() {
    try {
        const response = await fetch(`http://localhost:${LM_STUDIO_PORT}/`);
        return response.ok || response.status === 404; // 404 tambi√©n es v√°lido
    } catch (error) {
        return false;
    }
}

// Proxy din√°mico para la API OpenAI
app.use('/v1', createProxyMiddleware({
    target: () => `http://localhost:${LM_STUDIO_PORT}`,
    changeOrigin: true,
    timeout: 30000,
    proxyTimeout: 30000,
    onError: (err, req, res) => {
        console.error(`‚ùå Proxy error (puerto ${LM_STUDIO_PORT}):`, err.message);
        res.status(502).json({
            error: {
                message: 'LM Studio not available',
                type: 'proxy_error',
                code: 'lm_studio_unavailable',
                details: err.message,
                lm_studio_port: LM_STUDIO_PORT
            }
        });
    },
    onProxyReq: (proxyReq, req, res) => {
        console.log(`üì° Proxying ${req.method} ${req.url} to LM Studio:${LM_STUDIO_PORT}`);
    },
    onProxyRes: (proxyRes, req, res) => {
        console.log(`‚úÖ Response from LM Studio: ${proxyRes.statusCode}`);
    }
}));

// Fallback inteligente para /v1/models
app.get('/v1/models', async (req, res) => {
    console.log('üîÑ Fallback: Verificando LM Studio directamente...');
    
    const isAvailable = await checkLMStudio();
    if (!isAvailable) {
        // Intentar re-detectar el puerto
        console.log('üîç Re-detectando puerto de LM Studio...');
        LM_STUDIO_PORT = await detectLMStudioPort();
        
        const isAvailableAfterDetection = await checkLMStudio();
        if (!isAvailableAfterDetection) {
            return res.status(502).json({
                error: {
                    message: 'LM Studio server not responding',
                    type: 'connection_error',
                    code: 'lm_studio_down',
                    attempted_port: LM_STUDIO_PORT
                }
            });
        }
    }

    // Si LM Studio responde pero no tiene modelos, devolver respuesta mock
    res.json({
        object: "list",
        data: [
            {
                id: "nomic-embed-text-v1.5",
                object: "model",
                created: Math.floor(Date.now() / 1000),
                owned_by: "lmstudio"
            }
        ]
    });
});

// Health check mejorado con detecci√≥n autom√°tica
app.get('/health', async (req, res) => {
    const lmStudioStatus = await checkLMStudio();
    
    if (!lmStudioStatus) {
        // Intentar re-detectar puerto
        const newPort = await detectLMStudioPort();
        if (newPort !== LM_STUDIO_PORT) {
            LM_STUDIO_PORT = newPort;
            console.log(`üîÑ Puerto actualizado a: ${LM_STUDIO_PORT}`);
        }
    }
    
    const finalStatus = await checkLMStudio();
    
    res.json({ 
        status: 'ok', 
        timestamp: new Date().toISOString(),
        lm_studio_port: LM_STUDIO_PORT,
        lm_studio_available: finalStatus,
        proxy_version: '2.0.0'
    });
});

// Ruta ra√≠z
app.get('/', (req, res) => {
    res.json({
        message: 'LM Studio API Proxy',
        version: '2.0.0',
        features: ['auto-port-detection', 'intelligent-fallback', 'health-monitoring'],
        endpoints: ['/v1/models', '/v1/chat/completions', '/health'],
        lm_studio_port: LM_STUDIO_PORT
    });
});

// Inicializar servidor
async function startServer() {
    // Detectar puerto de LM Studio al iniciar
    console.log('üîç Detectando puerto de LM Studio...');
    LM_STUDIO_PORT = await detectLMStudioPort();
    
    app.listen(PORT, '0.0.0.0', () => {
        console.log(`üöÄ LM Studio API Proxy v2.0 running on port ${PORT}`);
        console.log(`üì° Auto-detected LM Studio on port ${LM_STUDIO_PORT}`);
        console.log(`üåê Available at: http://0.0.0.0:${PORT}`);
        console.log(`üîç Health check: http://0.0.0.0:${PORT}/health`);
        
        // Verificar LM Studio peri√≥dicamente
        setInterval(async () => {
            const isAvailable = await checkLMStudio();
            if (!isAvailable) {
                console.log(`‚ö†Ô∏è LM Studio no disponible en puerto ${LM_STUDIO_PORT}, re-detectando...`);
                const newPort = await detectLMStudioPort();
                if (newPort !== LM_STUDIO_PORT) {
                    LM_STUDIO_PORT = newPort;
                    console.log(`üîÑ Puerto actualizado a: ${LM_STUDIO_PORT}`);
                }
            }
        }, 30000); // Verificar cada 30 segundos
    });
}

startServer();